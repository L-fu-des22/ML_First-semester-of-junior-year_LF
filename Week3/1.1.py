```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')
import matplotlib.pyplot as plt
# import tensorflow as tf
from sklearn.metrics import classification_report#è¿™ä¸ªåŒ…æ˜¯è¯„ä»·æŠ¥å‘Š
import scipy.optimize as opt

# æŠŠExcelæ–‡ä»¶ä¸­çš„æ•°æ®è¯»å…¥pandas
data = pd.read_excel(r'E:\Junior first semester\374-Machine Learning System Design\week3\Homework3\Homework3\data\Disease\data_1.xlsx',)

sns.set(context="notebook", style="darkgrid", palette=sns.color_palette("RdBu", 2))
sns.lmplot('BP_HIGH', 'BMI', hue='HYPERTENTION', data=data,
           size=6,
           fit_reg=False,
           scatter_kws={"s": 50}
          )
plt.show()

def get_X(df):#è¯»å–ç‰¹å¾
#     """
#     use concat to add intersect feature to avoid side effect
#     not efficient for big dataset though
#     è¿”å›dataæ˜¯ä¸ª2ç»´çŸ©é˜µ
#     """
    ones = pd.DataFrame({'ones': np.ones(len(df))})#onesæ˜¯mè¡Œ1åˆ—çš„dataframe
    data = pd.concat([ones, df], axis=1)  # åˆå¹¶æ•°æ®ï¼Œæ ¹æ®åˆ—åˆå¹¶
    return np.array(data.iloc[:, :-1]) # è¿™ä¸ªæ“ä½œè¿”å› ndarray,ä¸æ˜¯çŸ©é˜µ

def get_y(df):#è¯»å–æ ‡ç­¾
#     '''
#     assume the last column is the target
#     è¿”å›yæ˜¯ä¸€ç»´çŸ©é˜µ
#     '''
    return np.array(df.iloc[:, -1])#df.iloc[:, -1]æ˜¯æŒ‡dfçš„æœ€åä¸€åˆ—

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cost(theta, X, y):
    ''' cost fn is -l(theta) for you to minimize'''
    return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))

def predict(x, theta):
    prob = sigmoid(x @ theta)
    return (prob >= 0.5).astype(int)

def gradient(theta, X, y):
#     '''just 1 batch gradient'''
    return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)

def feature_mapping(x, y, power, as_ndarray=False):
#     """return mapped features as ndarray or dataframe"""
    # data = {}
    # # inclusive
    # for i in np.arange(power + 1):
    #     for p in np.arange(i + 1):
    #         data["f{}{}".format(i - p, p)] = np.power(x, i - p) * np.power(y, p)

    data = {"f{}{}".format(i - p, p): np.power(x, i - p) * np.power(y, p)
                for i in np.arange(power + 1)
                for p in np.arange(i + 1)
            }

    if as_ndarray:
        return np.array(pd.DataFrame(data))
    else:
        return pd.DataFrame(data)

x1 = np.array(data['BP_HIGH'])

x2 = np.array(data['BMI'])

data_ft = feature_mapping(x1, x2, power=3)

theta = np.zeros(data_ft.shape[1])

X = feature_mapping(x1, x2, power=3, as_ndarray=True)

y = get_y(data)

def regularized_cost(theta, X, y, l=1):
#     '''you don't penalize theta_0'''
    theta_j1_to_n = theta[1:]
    regularized_term = (l / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum()

    return cost(theta, X, y) + regularized_term #æ­£åˆ™åŒ–ä»£ä»·å‡½æ•°

def regularized_gradient(theta, X, y, l=1):
#     '''still, leave theta_0 alone'''
# regularized gradient(æ­£åˆ™åŒ–æ¢¯åº¦)
    theta_j1_to_n = theta[1:]
    regularized_theta = (l / len(X)) * theta_j1_to_n

    # by doing this, no offset is on theta_0
    regularized_term = np.concatenate([np.array([0]), regularized_theta])

    return gradient(theta, X, y) + regularized_term

#ä½¿ç”¨ä¸åŒçš„  ğœ†  ï¼ˆè¿™ä¸ªæ˜¯å¸¸æ•°ï¼‰
#ç”»å‡ºå†³ç­–è¾¹ç•Œ

def draw_boundary(power, l):
#     """
#     power: polynomial power for mapped feature
#     l: lambda constant
#     """
    density = 300
    threshhold = 0.01

    final_theta = feature_mapped_logistic_regression(power, l)
    x, y = find_decision_boundary(density, power, final_theta, threshhold)
    df = pd.read_excel(r'E:\Junior first semester\374-Machine Learning System Design\week3\Homework3\Homework3\data\Disease\data_1.xlsx',)

    sns.lmplot('BP_HIGH', 'BMI', hue='HYPERTENTION', data=df, size=6, fit_reg=False, scatter_kws={"s": 100})

    plt.scatter(x, y, c='Red', s=1)
    plt.title('Decision boundary')
    plt.show()

def feature_mapped_logistic_regression(power, l):
#     """for drawing purpose only.. not a well generealize logistic regression
#     power: int
#         raise x1, x2 to polynomial power
#     l: int
#         lambda constant for regularization term
#     """
    df = pd.read_excel(r'E:\Junior first semester\374-Machine Learning System Design\week3\Homework3\Homework3\data\Disease\data_1.xlsx', )

    x1 = np.array(df.BP_HIGH)
    x2 = np.array(df.BMI)
    y = get_y(df)

    X = feature_mapping(x1, x2, power, as_ndarray=True)
    theta = np.zeros(X.shape[1])

    res = opt.minimize(fun=regularized_cost,
                       x0=theta,
                       args=(X, y, l),
                       method='TNC',
                       jac=regularized_gradient)

    final_theta = res.x
    print('The solution of the optimization is ')
    print(final_theta)
    # è®­ç»ƒé›†é¢„æµ‹æ¨¡å‹æŠ¥å‘Š
    y_pred = predict(X, final_theta)
    print('è®­ç»ƒé›†é¢„æµ‹æŠ¥å‘Šï¼š')
    print(classification_report(y, y_pred))

    # è¯»å–æµ‹è¯•é›†ï¼Œå¹¶æµ‹è¯•è®­ç»ƒé›†ç”Ÿæˆçš„æ¨¡å‹
    df_test = pd.read_excel(r'E:\Junior first semester\374-Machine Learning System Design\week3\Homework3\Homework3\data\Disease\data_1.xlsx', sheet_name='test')

    x1_test = np.array(df_test.BP_HIGH)
    x2_test = np.array(df_test.BMI)
    y_test = get_y(df_test)
    X_test = feature_mapping(x1_test, x2_test, power, as_ndarray=True)

    y_test_pred = predict(X_test, final_theta)
    print('æµ‹è¯•é›†é¢„æµ‹æŠ¥å‘Šï¼š')
    print(classification_report(y_test, y_test_pred))
    return final_theta

#å¯»æ‰¾å†³ç­–è¾¹ç•Œå‡½æ•°
def find_decision_boundary(density, power, theta, threshhold):
    t1 = np.linspace(100, 210, density)
    t2 = np.linspace(10, 40, density)

    cordinates = [(x, y) for x in t1 for y in t2]
    x_cord, y_cord = zip(*cordinates)
    mapped_cord = feature_mapping(x_cord, y_cord, power)  # this is a dataframe
    inner_product = np.array(mapped_cord) @ theta
    # [np.abs(inner_product) < threshhold] è¿”å›inner_productä¸­æ‰€æœ‰å°äºthreshholdç´¢å¼•
    decision = mapped_cord[np.abs(inner_product) < threshhold]

    return decision.f10, decision.f01

draw_boundary(power=3, l=5)#lambda=æƒ©ç½šç³»æ•°
```